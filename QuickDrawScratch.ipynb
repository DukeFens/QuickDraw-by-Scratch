{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbm09sGgOTFzaouLPh4n32",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DukeFens/QuickDraw-by-Scratch/blob/main/QuickDrawScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will build the convolution layer at first, but before it, we will discuss the math theory behind it. Let's see we have input with a picture of 28x28 and filter 3x3:"
      ],
      "metadata": {
        "id": "gtfUSkjqCvrQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "X =\n",
        "\\begin{bmatrix}\n",
        "x_{11} & x_{12} & \\cdots & x_{1,28} \\\\\n",
        "x_{21} & x_{22} & \\cdots & x_{2,28} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{28,1} & x_{28,2} & \\cdots & x_{28,28}\n",
        "\\end{bmatrix} \\; , \\; \\; K =\n",
        "\\begin{bmatrix}\n",
        "w_{11} & w_{12} & w_{13} \\\\\n",
        "w_{21} & w_{22} & w_{23} \\\\\n",
        "w_{31} & w_{32} & w_{33}\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "wjGaBTcIE_vt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "770e0721"
      },
      "source": [
        "We apply the convolution for X and K, that means we slide the kernel onto X and for each patch (m, n) while sliding we calculate:\n",
        "\\begin{align*}\n",
        "a_{m,n} &= \\sum_{i=0}^{2} \\sum_{j=0}^{2} x_{m+i, n+j} w_{i, j} \\\\\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To avoid the shape reducing, before convolution, we add zeros around the border of X, then Z becomes:\n",
        "$$\n",
        "Z = X * K + b = \\begin{bmatrix}\n",
        "a_{1,1} & a_{1,2} & \\cdots \\\\\n",
        "\\vdots & \\ddots & \\cdots \\\\\n",
        "a_{28, 1} & \\cdots & a_{28, 28}\n",
        "\\end{bmatrix} \\text{ (we also add bias)}\n",
        "$$"
      ],
      "metadata": {
        "id": "K9ag8SLcLSjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then apply ReLu function to get the final feature map, we can see in general:"
      ],
      "metadata": {
        "id": "QKdzxLRUOgyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "A = ReLu(Z) = ReLu(X * K + b) \\text{, with stride 1 and padding 1.}\n",
        "$$"
      ],
      "metadata": {
        "id": "O7EdVeiYOqS3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We increase the strong features by applying pooling 2x2 onto A with stride 2, then we have pooling activated feature map:\n",
        "$$\n",
        "P = \\begin{bmatrix}\n",
        "p_{1,1} & p_{1,2} & \\cdots \\\\\n",
        "\\vdots & \\ddots & \\cdots \\\\\n",
        "p_{14, 1} & \\cdots & p_{14, 14}\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "sXOcMh84PZA0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cce3914"
      },
      "source": [
        "We have done convolution layer 1 sofar, now let's see in general our network design:\n",
        "\n",
        "$$\n",
        "X \\xrightarrow{\\text{conv}} Z^{[1]} \\xrightarrow{\\text{ReLu}} A^{[1]} \\xrightarrow{\\text{pooling}} P^{[1]} \\xrightarrow{\\text{conv}} Z^{[2]} \\xrightarrow{\\text{ReLu}} A^{[2]} \\xrightarrow{\\text{pooling}} P^{[2]}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16678f50"
      },
      "source": [
        "We have 2 layer for convolution, notice that the shape change by:\n",
        "$$\n",
        "m = \\frac{n+2p-f+1}{s}\n",
        "$$\n",
        "Which:\n",
        "  - $m$: Output size\n",
        "  - $n$: Input size\n",
        "  - $p$: Padding size\n",
        "  - $f$: Filter size or pooling size\n",
        "  - $s$: Stride size"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we have pooling activated feature map in layer 2:\n",
        "$$\n",
        "P^{[2]} = \\begin{bmatrix}\n",
        "p_{1,1} & p_{1,2} & \\cdots \\\\\n",
        "\\vdots & \\ddots & \\cdots \\\\\n",
        "p_{7, 1} & \\cdots & p_{7, 7}\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "oH2Z4g2-TLo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, more crazy, we have channel 1 for gray color of image and **32 filters** for layer 1, **64 filters** for layer 2 to detect more features. Then, the general mathematics looks like:"
      ],
      "metadata": {
        "id": "ib7HVNbacsZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "X =\n",
        "\\begin{bmatrix}\n",
        "x_{11} & x_{12} & \\cdots & x_{1,28} \\\\\n",
        "x_{21} & x_{22} & \\cdots & x_{2,28} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{28,1} & x_{28,2} & \\cdots & x_{28,28}\n",
        "\\end{bmatrix} \\; , \\; \\; K_1 =\n",
        "\\begin{bmatrix}\n",
        "k_1 & k_2 & \\cdots & k_{32}\n",
        "\\end{bmatrix}, K_2 =\n",
        "\\begin{bmatrix}\n",
        "k_1 & k_2 & \\cdots & k_{64}\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "C7uPiU24dcGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "which for each $k_i$:\n",
        "$$\n",
        "k_{i}=\\begin{pmatrix}\n",
        "w_{i11} & w_{i12} & w_{i13} \\\\\n",
        "w_{i21} & w_{i22} & w_{i23} \\\\\n",
        "w_{i31} & w_{i32} & w_{i33}\n",
        "\\end{pmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "Z5s9uZjLc5kC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we calculate for i in $K_1$:"
      ],
      "metadata": {
        "id": "8Qol5ezTeZOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{align*}\n",
        "z_i &= X * k_i + b_1 \\\\\n",
        "a_i &= ReLu(z_i) \\\\\n",
        "p_i &= pooling(a_i) \\\\\n",
        "\\end{align*} \\\\\n",
        "\\therefore P^{[1]} \\text{ has shape (32, 14,14) as the channel replaced by number of filters} \\\\\n",
        "\\therefore P^{[2]} \\text{ is (64, 7, 7) as the same calculating.}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "jQW68EuVerSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We flatten it to get input for full connected layer as we can see **64x7x7=3136** and suppose we have n examples and 128 perceptron in dense layer 1, then:\n",
        "$$\n",
        "X_{FC} =\n",
        "\\begin{bmatrix}\n",
        "x_{11} & x_{12} & \\cdots & x_{1,3136} \\\\\n",
        "x_{21} & x_{22} & \\cdots & x_{2,3136} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_{n,1} & x_{n,2} & \\cdots & x_{n,3136}\n",
        "\\end{bmatrix} \\; , \\; \\; W^{[1]} =\n",
        "\\begin{bmatrix}\n",
        "w_{11} & w_{12} & \\cdots & w_{1,128} \\\\\n",
        "w_{21} & w_{22} & \\cdots & w_{2,128} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "w_{3136,1} & w_{3136,2} & \\cdots & w_{3136,128}\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "85N0IMU6hk9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\begin{align*}\n",
        "\\therefore Z^{[1]} &= XW + b^{[1]} \\\\\n",
        "\\therefore A^{[1]} &= ReLu(Z^{[1]})\n",
        "\\end{align*}\n",
        "$$"
      ],
      "metadata": {
        "id": "s4YbbCV4frJz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f7c34de"
      },
      "source": [
        "For layer 2, we have:\n",
        "\n",
        "$Softmax$: $$ \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} $$\n",
        "- $K$: number of classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8103c93b"
      },
      "source": [
        "Then:\n",
        "$$\n",
        "A^{[2]} = Softmax(Z^{[2]}) = Softmax(Z^{[1]}W^{[2]} + b^{[2]})\n",
        "$$\n",
        "Which:\n",
        "- $W^{[2]}$: 5 percentrons equals to 5 class of output.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_7UPMnH5mM9Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}